Requirement: Internal red-teaming
OpenAI has conducted internal adversarial testing as part of their red teaming efforts, which involves stress-testing AI systems by simulating real-world challenges and vulnerabilities. This process has evolved to include collaborations with external experts to develop domain-specific taxonomies of risk and evaluate potentially harmful capabilities in new systems.

For more detailed information on OpenAI's internal red teaming efforts, you can refer to the following links:
- https://openai.com/index/red-teaming-network/
- https://openai.com/global-affairs/response-to-nist-executive-order-on-ai/
- https://www.automationinside.com/article/openai-red-teaming-network

--------------------------------------------------

Requirement: External red-teaming
OpenAI has engaged in external red-teaming through various initiatives, including:

1. **OpenAI Red Teaming Network**: This network involves collaborating with external experts to rigorously evaluate and red team AI models, focusing on domain-specific taxonomies of risk and evaluating potentially harmful capabilities in new systems.
2. **Red Teaming Campaigns**: OpenAI has conducted red teaming campaigns with external experts to assess vulnerabilities and limitations of models like GPT-4 and DALL-E 3.
3. **Researcher Access Program**: This program allows researchers to access OpenAI's models for external assessments, which is part of their broader red teaming efforts.

These initiatives demonstrate OpenAI's commitment to external red-teaming to improve the safety of their AI models.

- **Links**:
  - https://www.automationinside.com/article/openai-red-teaming-network
  - https://forum.openai.com/public/events/virtual-event-red-teaming-ai-systems-2024
  - https://openai.com/index/red-teaming-network/

--------------------------------------------------

Requirement: Red teaming related to each of the risk areas
OpenAI has engaged in various red teaming efforts to assess risks associated with their AI models. Here are some key activities:

1. **Comprehensive Evaluation**: OpenAI conducts thorough evaluations of their AI models to identify potential risks, biases, and vulnerabilities. This includes assessing risks such as the development of nuclear, radiological, biological, and chemical weapons (CBRN), cyber risks, and self-replication capabilities.

2. **Diverse Perspectives**: The OpenAI Red Teaming Network involves experts from various fields, including machine learning, ethics, cybersecurity, and policy, to ensure a holistic evaluation process.

3. **Continuous Improvement**: OpenAI uses feedback and insights gained through the red teaming process to refine and enhance the safety and reliability of their AI models continually.

4. **Transparency and Accountability**: The network promotes transparency in AI development by allowing external experts to hold OpenAI accountable for its safety practices and ethical considerations.

5. **Red Teaming Specific Risks**:
   - **CBRN Risks**: OpenAI red teamed DALL-E 3 to assess its ability to provide visual information needed to develop, acquire, or disperse CBRN.
   - **Cyber Risks**: Red teamers tested GPT-4 for increased cyber risk.
   - **Self-Replication Capabilities**: Red teaming was conducted to evaluate the potential for self-replication in AI models.

6. **Iterative Red Teaming**: OpenAI documents key findings from red teaming efforts in System Cards, making these learnings publicly available for others to build upon.

### Links Used:
- https://www.itbriefcase.net/enhancing-ai-safety-openais-red-teaming-network
- https://openai.com/global-affairs/our-approach-to-frontier-risk/
- https://openai.com/index/red-teaming-network/
- https://openai.com/global-affairs/response-to-nist-executive-order-on-ai/

--------------------------------------------------

Requirement: Info sharing with companies
OpenAI has implemented several measures to ensure data privacy and security, particularly for companies, including:

1. **Data Processing Addendum**: OpenAI offers a Data Processing Addendum for customers to ensure compliance with privacy laws like GDPR and CCPA.
2. **Enterprise-grade Security**: ChatGPT Enterprise provides enterprise-grade security and privacy, allowing companies to own and control their business data. It does not train on business data and encrypts all conversations.
3. **Zero Data Retention**: For trusted customers with sensitive applications, zero data retention is available, ensuring that request and response bodies are not persisted to any logging mechanism.

### Links:
- [OpenAI's Security and Privacy Commitment](https://openai.com/security/)
- [Enterprise Privacy at OpenAI](https://openai.com/enterprise-privacy/)
- [Does the OpenAI API Get Access to the Data I Send It?](https://community.openai.com/t/does-the-openai-api-get-access-to-the-data-i-send-it-or-store-the-data/599538)

--------------------------------------------------

Requirement: Info sharing with government
OpenAI has shared its AI models with the US government for safety testing and has engaged in information sharing regarding trust and safety risks. This includes:

- **Providing access to AI models**: OpenAI and Anthropic have agreed to give the US government access to their latest models for safety evaluations.
- **Collaboration with government agencies**: OpenAI is working with USAID and Los Alamos National Laboratory, and has also supported the GSA Federal AI Hackathon and the FHFA Generative AI Tech Sprint.
- **Voluntary commitments**: OpenAI is part of voluntary commitments to reinforce AI safety, security, and trustworthiness through information sharing and common standards.

### Links:
- https://www.barrons.com/news/openai-and-anthropic-to-share-ai-models-with-us-government-ebf94f81
- https://fedscoop.com/openai-chatgpt-enterprise-usaid/
- https://openai.com/index/moving-ai-governance-forward/

--------------------------------------------------

Requirement: Establish or join a forum or mechanism
OpenAI has established or joined the Frontier Model Forum, an industry body focused on ensuring the safe and responsible development of frontier AI models. The Forum aims to advance AI safety research, identify safety best practices, share knowledge with policymakers, and support efforts to leverage AI to address societal challenges.

- **Frontier Model Forum**: https://openai.com/index/frontier-model-forum/
- **Microsoft, Anthropic, Google, and OpenAI launch Frontier Model Forum**: https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum/

--------------------------------------------------

Requirement: Sharing information in the specific risk areas
OpenAI has taken several steps to address specific risk areas by sharing information and implementing safety measures:

1. **Publishing System Cards**: OpenAI publishes system cards for new AI systems, providing detailed information about the model, its capabilities, and safety considerations. This helps in transparency and accountability.
2. **Red-Teaming**: OpenAI conducts pre-deployment safety evaluations and red-teaming to identify potential risks and vulnerabilities before releasing new models.
3. **Content Detection and Monitoring**: Azure OpenAI Studio provides a Risks & Safety monitoring dashboard that includes content detection and potentially abusive user detection, allowing for proactive measures to mitigate risks.
4. **Voluntary Commitments**: OpenAI has made voluntary commitments to promote safety, security, and trust in AI, including the development of a Preparedness Framework and participation in forums like the Frontier Model Forum.

### Links:
- [OpenAI's Approach to Frontier Risk](https://openai.com/global-affairs/our-approach-to-frontier-risk/)
- [How to use Risks & Safety monitoring in Azure OpenAI Studio](https://learn.microsoft.com/ja-jp/azure/ai-services/openai/how-to/risks-safety-monitor)
- [Usage policies - OpenAI](https://openai.com/policies/usage-policies/)

--------------------------------------------------

Requirement: Invest in cybersecurity / store and working with the weights in an appropriately secure environment
OpenAI has invested in cybersecurity through its Cybersecurity Grant Program, which aims to equip cyber defenders with advanced AI models and foster innovative research at the nexus of cybersecurity and AI. This program includes initiatives such as:

- **Supporting Research Projects**: OpenAI has supported various research projects, including those focused on defending against prompt-injection attacks in large language models (LLMs) and developing AI-specific audit and compliance programs.
- **Granting Access to AI Tools**: OpenAI has granted free access to ChatGPT Plus to many in the cybersecurity community, enhancing AI adoption in cyber defense.
- **Developing Secure Infrastructure**: OpenAI has outlined six security measures for advanced AI, including trusted computing for AI accelerators, network and tenant isolation guarantees, and AI for cyber defense.

These efforts demonstrate OpenAI's commitment to enhancing cybersecurity through the application of AI.

Links:
- https://openai.com/index/empowering-defenders-through-our-cybersecurity-grant-program/
- https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/

--------------------------------------------------

Requirement: Establish a robust insider threat detection program
OpenAI has established a robust insider threat detection program by:

1. **Hiring a Technical Insider Risk Investigator**: To detect, analyze, and mitigate potential insider threats by correlating data from various sources and developing, maintaining, and enhancing insider threat indicators.
2. **Implementing Insider Risk Investigator Roles**: Including both Technical and Open Source Insider Risk Investigators to conduct in-depth research, write comprehensive analytic reports, and perform investigations.
3. **Enhancing Cybersecurity Measures**: Investing in cybersecurity and insider threat safeguards, including a bug bounty program to recognize and reward individuals who report security vulnerabilities.
4. **Collaborating with the White House**: Committing to investing in cybersecurity and insider threat safeguards following meetings with the Biden administration to discuss AI risks.

### Links:
- https://www.ccn.com/news/technology/openai-hiring-insider-risk-investigator-collaborating-white-house/
- https://openai.com/careers/technical-insider-risk-investigator/
- https://openai.com/careers/open-source-insider-risk-investigator/
- https://openai.com/global-affairs/our-approach-to-frontier-risk/

--------------------------------------------------

Requirement: Limiting access to model weights to those whose job function requires it
OpenAI has limited access to model weights through its API, which allows access to model outputs and fine-tuning procedures but does not provide direct access to the model weights themselves. This approach restricts the applications of the model and limits the speed at which model outputs can be accessed, thereby controlling who can use the model and how it is used.

- **API Limitations**: OpenAI's API limits direct access to model weights, allowing only controlled access through monitoring and review processes.
- **Model Weight Access**: OpenAI has not released model weights for GPT-3.5, citing safety and ethical concerns.
- **Security Measures**: OpenAI's approach to securing model weights includes rigorous assessment protocols and testing for various modifications, including fine-tuning, as part of their Preparedness Framework.

--------------------------------------------------

Requirement: Establish bounties, contests, or prizes
OpenAI has established a bug bounty program and a feedback contest to encourage security researchers and enthusiasts to identify and report vulnerabilities in their systems. Here are the details:

- **Bug Bounty Program**: OpenAI has launched a bug bounty program with cash rewards ranging from $200 to $20,000 for reported vulnerabilities. The program is managed by Bugcrowd and includes targets such as OpenAI API, ChatGPT, and corporate information. The program aims to ensure the security and reliability of OpenAI's technology.
- **Feedback Contest**: OpenAI has also run a ChatGPT Feedback Contest where participants could submit feedback on problematic outputs from ChatGPT. The contest offered $500 in API credits as prizes and aimed to improve OpenAI's understanding of risks and harms associated with their models.

### Links Used:
1. https://openai.com/index/bug-bounty-program/
2. https://therecord.media/chatgpt-maker-openai-bug-bounty-program
3. https://cdn.openai.com/chatgpt/chatgpt-feedback-contest.pdf
4. https://www.csoonline.com/article/575007/openai-starts-bug-bounty-program-with-cash-rewards-up-to-20000.html
5. https://rethinkpriorities.org/publications/ai-safety-bounties

--------------------------------------------------

Requirement: Include AI systems in their existing bug bounty programs
OpenAI has announced a bug bounty program for its AI systems, including ChatGPT, which offers cash rewards ranging from $200 to $20,000 for identifying vulnerabilities. This program is managed by Bugcrowd and includes OpenAI's applications, website, documentation, and API keys.

- **OpenAI's Bug Bounty Program**: Detailed guidelines and rules for participation can be found on their Bug Bounty Program page.
- **Rewards and Incentives**: The program offers cash rewards based on the severity and impact of the reported issues, ranging from $200 for low-severity findings to up to $20,000 for exceptional discoveries.
- **Partnership with Bugcrowd**: OpenAI has partnered with Bugcrowd to manage the submission and reward process, ensuring a streamlined experience for all participants.

--------------------------------------------------

Requirement: Robust provenance or watermarking for audio
OpenAI has implemented several measures related to robust provenance and watermarking for audio:

1. **Audio Watermarking**: OpenAI has developed a robust watermarking technique that embeds invisible signals into audio content, such as clips from Voice Engine, their proprietary text-to-speech platform.

2. **Metadata Embedding**: OpenAI is exploring metadata embedding as a method to ensure digital content authenticity. This involves embedding cryptographic data within AI-generated content, making it less prone to tampering.

3. **C2PA Standards**: OpenAI has joined the Coalition for Content Provenance and Authenticity (C2PA) and is working to integrate content credentials into image metadata, which can serve as watermarks providing details about the image's ownership and creation process.

**Links:**
- https://content-whale.com/blog/openai-watermarking-plan-scrapped/
- https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online/
- https://www.newsbytesapp.com/news/science/openai-develops-advanced-tools-for-image-verification-audio-watermarking/story

--------------------------------------------------

Requirement: Robust provenance or watermarking for visual content
OpenAI has implemented several measures for robust provenance and watermarking of visual content:

1. **Provenance Classifier**: OpenAI is developing an AI tool to identify images created by its DALL-E 3 image generator, which is over 99% accurate at identifying unmodified images and over 95% accurate with modified images.
2. **C2PA Metadata**: OpenAI is incorporating C2PA (Coalition for Content Provenance and Authenticity) metadata into its products, ensuring that edited images retain their provenance information, including details like the app and tool used, actions taken, and other modifications.
3. **Tamper-Resistant Watermarking**: OpenAI is implementing tamper-resistant watermarking for digital content, including audio, to make it hard to remove.

These measures aim to enhance the integrity and transparency of digital content generated by OpenAI's tools.

- https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online/
- https://www.etcentric.org/openai-developing-provenance-classifier-for-genai-images/
- https://openai.com/index/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise/

--------------------------------------------------

Requirement: Develop tools or APIs to determine if a particular piece of content was created within their tools
OpenAI has developed tools and APIs to determine if a particular piece of content was created within their tools, including:

1. **OpenAI Text Classifier**: This tool classifies text based on the likelihood that it was created by artificial intelligence, ranging from "very unlikely" to "likely" AI-generated.
2. **Image Detection Classifier**: This tool predicts the likelihood that an image was generated by OpenAI's DALL·E 3, aiming to enhance the integrity of digital content.
3. **C2PA (Content Creators, Publishers, and Authenticators) metadata**: OpenAI incorporates C2PA metadata to track the history and modifications of digital content, ensuring transparency.

For more detailed information, refer to:
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Understanding the source of what we see and hear online](https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online/)
- [Does the OpenAI Classifier have an API? AI Detection Tools](https://community.openai.com/t/does-the-openai-classifier-have-an-api-ai-detection-tools/168179)

--------------------------------------------------

Requirement: Work with industry peers and standards-setting bodies as appropriate towards developing a technical framework to help users distinguish audio or visual content generated by users from audio or visual content generated by AI
OpenAI has contributed to the development of a technical framework for distinguishing between human-generated and AI-generated content through various initiatives:

1. **Improving Verifiability in AI Development**: OpenAI has contributed to a multi-stakeholder report that describes 10 mechanisms to improve the verifiability of claims made about AI systems, including audit trails, red teaming exercises, and bias and safety bounties.

2. **Supplier Code of Conduct**: While not directly focused on distinguishing between human and AI-generated content, OpenAI's Supplier Code of Conduct emphasizes ethical conduct, transparency, and data integrity, which are essential for ensuring the authenticity of content.

3. **Safety by Design Principles**: OpenAI has engaged in discussions about safety by design principles, which involve developing safeguards to prevent harmful content generation. This includes monitoring and detecting suspicious activity on their platforms.

These efforts collectively contribute to the development of a technical framework that can help users distinguish between human and AI-generated content. 

**Links:**
- [Improving Verifiability in AI Development](https://openai.com/index/improving-verifiability/)
- [OpenAI Supplier Code of Conduct](https://openai.com/policies/supplier-code/)
- [Safety by Design Principles](https://www.thorn.org/blog/a-safety-by-design-conversation-with-thorn-all-tech-is-human-google-openai-and-stabilityai/)

--------------------------------------------------

Requirement: Report capabilities
OpenAI has reported the capabilities of its models through various technical reports and system cards. Key points include:

- **GPT-4 Technical Report**: Describes GPT-4's performance on various NLP tasks, its multimodal capabilities, and its ability to follow user intent.
- **GPT-4o System Card**: Details the training data and capabilities of GPT-4o, including its multimodal data and safety evaluations.
- **OpenAI o1-preview**: Introduces the o1 series of reasoning models, highlighting their enhanced capabilities in complex tasks like coding and math, and their safety training approach.

### Links:
1. [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)
2. [GPT-4o System Card](https://openai.com/index/gpt-4o-system-card/)
3. [OpenAI o1-preview](https://openai.com/index/introducing-openai-o1-preview/)

--------------------------------------------------

Requirement: Report limitations
OpenAI has faced several limitations and issues that users have reported, including:

1. **Inability to Respect Spending Limits**: Users have reported that OpenAI does not honor spending limits set by users, leading to unexpected charges. For example, a user was charged $1200 despite setting a hard limit of $50.

2. **GPT-4 Limitations**: GPT-4 has limitations such as speed and cost issues for applications with significant user interactions. It also experiences hallucinations, which are fabrications presented as truthful or factual.

3. **ChatGPT Limitations**: ChatGPT has several limitations, including:
   - Lack of real-time information due to training data only going up until September 2021.
   - Inability to access the internet or real-time data.
   - Biases in the training data.
   - Limited domain expertise.

4. **API Response Limits**: The OpenAI API has a 4096 token limit for response output, which can be a constraint for generating lengthy documents. Users have also noted that the model's internal training limits the output length, making it difficult to exceed 500 words in a single API call.

5. **File Upload Limits**: There is a 16MB limit for file uploads, but larger files can be handled through Blob storage and an offline ingestion script.

### Links Used:
- https://community.openai.com/t/exceeding-budget-constraints-charged-1200-by-openai-despite-setting-spending-limits/698204
- https://community.openai.com/t/time-to-talk-about-gpt-4-limitation/516216
- https://www.linkedin.com/pulse/4-openai-limitations-according-chatgpt-alexandra-lazowska
- https://community.openai.com/t/4096-response-limit-vs-128-000-context-window/656864
- https://learn.microsoft.com/en-us/answers/questions/1362205/i-want-to-know-limitation-related-to-openai-servic

--------------------------------------------------

Requirement: Report domains of appropriate and inappropriate use
OpenAI has taken steps to monitor and report domains of inappropriate use, particularly those involving sensitive or harmful content. Here are the relevant actions:

1. **Monitoring and Reporting Inappropriate Content**:
   - OpenAI uses a combination of automated systems and human review to identify and enforce against misuse of their models, including reporting apparent child sexual abuse material (CSAM) to the National Center for Missing and Exploited Children.
   - They also use Moderation API to check for policy-violating input prompts and filter out harmful content.

2. **Enforcing Domain Name Usage**:
   - OpenAI has been sending letters to end users using domain names ending in "GPT" to cease their use, indicating their disapproval of certain domain names and their potential to revoke access to their API.

These actions demonstrate OpenAI's efforts to monitor and report domains of inappropriate use, ensuring compliance with their policies and safety standards.

- **Links**:
  - https://openai.com/policies/usage-policies/
  - https://community.openai.com/t/can-we-control-inappropriate-content-does-dall-e-usages-copyrighted-images-to-generate-new-images/386867
  - https://platform.openai.com/docs/guides/safety-best-practices
  - https://www.namepros.com/threads/openai-pushing-back-against-gpt-domains.1301278/

--------------------------------------------------

Requirement: Publish transparency reports
OpenAI has published transparency reports and system cards that outline the safety work and capabilities of their models. This includes:

- **GPT-4o System Card**: Details the safety work carried out prior to releasing GPT-4o, including external red teaming and frontier risk evaluations.
- **OpenAI o1 System Card**: Outlines the safety work carried out prior to releasing OpenAI o1-preview and o1-mini, including external red teaming and frontier risk evaluations.
- **Transparency Note for Azure OpenAI Service**: Provides an overview of how Azure OpenAI models work, their capabilities, and limitations, aiming to help developers and users understand the technology and its potential impacts.

These reports and system cards are available on OpenAI's Trust Portal.

--------------------------------------------------

Requirement: Report safety evaluations
