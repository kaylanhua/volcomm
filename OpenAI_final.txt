Requirement: Internal red-teaming
OpenAI has taken several steps since the beginning of 2023 to enhance the safety of their AI models through internal and external red-teaming efforts:

1. **OpenAI Red Teaming Network**: OpenAI launched an open call for domain experts to join the OpenAI Red Teaming Network, aimed at improving the safety of their AI models. This network involves rigorous evaluation and red-teaming of AI models by external experts from various fields, including machine learning, ethics, cybersecurity, and policy.

2. **External Red Teaming**: OpenAI conducted extensive external red-teaming for the GPT-4o API, involving over 100 external red teamers from 29 different countries. This process included four phases of testing, from early model checkpoints to final model candidates, focusing on identifying risks and stress-testing mitigations.

3. **Iterative Red Teaming**: OpenAI has documented several red-teaming efforts for frontier model launches, including those for DALLÂ·E 2 and GPT-4. These efforts involve expert red teamers accessing pre-trained models with varying degrees of fine-tuning and post-training safety mitigations. The insights from red-teaming inform the development of post-training level mitigations, system-level mitigations, policies, and evaluations.

### Links Used:
- https://openai.com/global-affairs/response-to-nist-executive-order-on-ai/
- https://openai.com/index/gpt-4o-system-card/
- https://www.itbriefcase.net/enhancing-ai-safety-openais-red-teaming-network
- https://openai.com/index/red-teaming-network/

--------------------------------------------------

Requirement: External red-teaming
OpenAI has engaged in several external red-teaming efforts since the beginning of 2023, including:

1. **OpenAI Red Teaming Network**: OpenAI has launched an open call for domain experts to join their Red Teaming Network, aiming to rigorously evaluate and assess the safety of their AI models. This network involves experts from various fields, including machine learning, ethics, cybersecurity, and policy, to simulate real-world challenges and vulnerabilities in AI systems.

2. **GPT-4o System Card**: OpenAI conducted extensive external red-teaming for the GPT-4o model, involving over 100 external red teamers from 45 different languages and 29 countries. This effort included four phases of testing, starting from early model checkpoints to final model candidates, focusing on assessing novel risks and stress-testing safety mitigations.

3. **Response to NIST Executive Order**: OpenAI has emphasized the importance of red teaming in evaluating and auditing AI capabilities, particularly in response to the NIST Executive Order on AI. This includes conducting red teaming tests to identify potential risks associated with new models and systems, especially those requiring domain-specific knowledge and context.

--------------------------------------------------

Requirement: Red teaming related to each of the risk areas
OpenAI has made several significant announcements and developments since the beginning of 2023 that might fit under "Red teaming related to each of the risk areas." Here are the key points:

1. **OpenAI DevDay 2023**:
   - **GPT-4 Turbo**: Introduced a more capable model with a 128K token context window and more affordable pricing.
   - **Assistants API**: A tool for building sophisticated and interactive AI-driven applications.
   - **Custom GPTs and GPT Store**: A collaborative AI marketplace with revenue sharing, allowing developers to create and share custom GPT models.
   - **Custom Models**: Offers fine-grain control over AI behavior.
   - **Copyright Shield**: Provides legal protection to users of the OpenAI platform, addressing the complex legalities associated with AI-generated content.

2. **Breakout Sessions**:
   - Detailed sessions on AI agents and APIs, large language models, fine-tuning techniques, and the journey of AI applications from concept to production-ready solutions.

3. **Keynote Live Stream**:
   - Sam Altman's keynote unveiled the newest OpenAI innovations with live demos, including new models and pricing updates.

These announcements highlight OpenAI's focus on advancing AI capabilities, improving developer tools, and addressing legal and ethical concerns.

### Links Used:
- [OpenAI DevDay 2023: 5 Key Takeaways from the conference](https://www.pluralsight.com/resources/blog/ai-and-data/openai-devday-2023-takeaways)
- [OpenAI Dev-Day 2023: Breakout Sessions - Community](https://community.openai.com/t/openai-dev-day-2023-breakout-sessions/505213)
- [OpenAI Dev-Day 2023: Announcement - Community](https://community.openai.com/t/openai-dev-day-2023-announcement/472706)

--------------------------------------------------

Requirement: Info sharing with companies
Since the beginning of 2023, OpenAI has implemented several measures related to info sharing with companies, including:

1. **Zero Data Retention (ZDR) Policy**: OpenAI offers a ZDR policy for trusted customers with sensitive applications, ensuring that request and response bodies are not persisted to any logging mechanism and exist only in memory to serve the request.

2. **Enterprise Privacy Commitments**: OpenAI commits to privacy and security for its API Platform and ChatGPT Enterprise, ensuring that business data is not trained on and that customers own and control their data.

3. **Data Deletion Requests**: OpenAI allows customers to request the deletion of their content, which will be processed within 30 days, and ensures that data is not shared with third parties for marketing purposes.

These measures aim to protect the data and privacy of companies using OpenAI's services.

### Links:
- https://community.openai.com/t/does-the-openai-api-get-access-to-the-data-i-send-it-or-store-the-data/599538
- https://community.openai.com/t/using-private-company-data-with-openai-chatgpt/474233
- https://openai.com/enterprise-privacy/

--------------------------------------------------

Requirement: Info sharing with government
OpenAI has taken several steps since the beginning of 2023 that involve information sharing with the government:

1. **Agreement with the US AI Safety Institute**: OpenAI and Anthropic agreed to give the US AI Safety Institute access to major new AI models to help identify safety risks and solutions. This initiative aims to enhance AI safety and facilitate collaboration between the US and UK in assessing safety concerns.

2. **Regulatory Compliance and Safety Protocols**: OpenAI has been involved in discussions and agreements regarding AI safety protocols. For example, the company has participated in closed-door forums orchestrated by Senate Majority Leader Chuck Schumer to deliberate on AI's benefits and risks. Additionally, the White House issued a comprehensive executive order on AI in October, outlining new safety standards and mandating companies to notify the government during the training process.

3. **Canadian Privacy Investigation**: OpenAI is under investigation by Canadian privacy authorities for allegedly collecting, using, and disclosing personal information without consent through ChatGPT. The investigation will examine whether OpenAI received "valid and meaningful" information-sharing consent from Canadian users.

### Links Used:
- https://thehill.com/policy/technology/4323378-chatgpt-controversey-first-year-openai-sam-altman/
- https://www.theverge.com/2024/8/29/24231395/openai-anthropic-share-models-us-ai-safety-institute
- https://www.cbc.ca/news/canada/british-columbia/canada-privacy-investigation-chatgpt-1.6854468

--------------------------------------------------

Requirement: Establish or join a forum or mechanism
OpenAI has not explicitly established or joined a new forum or mechanism since the beginning of 2023. However, they have engaged with their community through their forums and have made several announcements and updates:

1. **Community Engagement**: OpenAI has been active in their community forums, addressing user concerns and providing updates on their products and services.
2. **Product Announcements**: They have introduced new products and features, such as the GPT-4o and improvements to the fine-tuning API.
3. **Partnerships**: OpenAI has partnered with companies like BuzzFeed to integrate their AI technology into content generation.

These activities reflect their ongoing engagement with users and the broader AI community, but they do not specifically involve establishing or joining a new forum or mechanism.

### Links Used:
- https://community.openai.com/t/now-thats-behind-us-can-openai-stop-competing-with-its-developers/519985
- https://openai.com/news?author=tom-brown
- https://timelines.issarice.com/wiki/Timeline_of_OpenAI

--------------------------------------------------

Requirement: Sharing information in the specific risk areas
OpenAI has taken several steps since the beginning of 2023 that might fit under "Sharing information in specific risk areas":

1. **Announcement of a new classifier to distinguish between human-written and AI-generated text**:
   - This classifier was made publicly available to inform mitigations for false claims that AI-generated text was written by a human.

2. **Formation of a Safety and Security Committee**:
   - The OpenAI Board formed a Safety and Security Committee to address concerns related to AI safety and security.

3. **Partnership with Consensus for scientific web search and generative AI**:
   - OpenAI partnered with Consensus, a Boston-based AI-powered search engine, to explore advancements in scientific web search and generative AI.

4. **Extension of partnership with Microsoft**:
   - OpenAI and Microsoft extended their partnership with a multi-billion dollar investment to continue research and development of safe, useful, and powerful AI.

5. **Release of a new classifier for identifying AI-generated text**:
   - OpenAI made a new classifier publicly available to help educators and researchers identify AI-generated text, although it has limitations and is not fully reliable.

### Links Used:
- https://openai.com/news?author=tom-brown
- https://timelines.issarice.com/wiki/Timeline_of_OpenAI
- https://timelines.issarice.com/wiki/Timeline_of_OpenAI#2023

--------------------------------------------------

Requirement: Invest in cybersecurity / store and working with the weights in an appropriately secure environment
OpenAI has taken several steps to enhance cybersecurity since the beginning of 2023:

1. **Launched the Cybersecurity Grant Program**: A $1M initiative to boost and quantify AI-powered cybersecurity capabilities, fostering high-level AI and cybersecurity discourse.
2. **Established the Cybersecurity Grant Program**: Empowering defenders by ensuring cutting-edge AI capabilities benefit them first, measuring cybersecurity capabilities, and elevating discourse at the intersection of AI and cybersecurity.
3. **Supported Various Cybersecurity Projects**: Including projects like those by Wagner Lab at UC Berkeley, Coguard, Mithril Security, and the Breuer Lab at Dartmouth, which focus on defending against prompt-injection attacks, reducing software misconfigurations, fortifying inference infrastructure for LLMs, and developing new defense techniques against attacks on neural networks.
4. **Thwarted Misuse Attempts**: OpenAI revealed that it had thwarted five undisclosed attempts to misuse its AI models for fraudulent activities online, highlighting concerns about potential misuse and the need for enhanced security measures.

### Links Used:
- https://grantsforus.io/international-grants/2023-openai-cybersecurity-grant-program/
- https://openai.com/index/openai-cybersecurity-grant-program/
- https://openai.com/index/empowering-defenders-through-our-cybersecurity-grant-program/
- https://www.reuters.com/technology/cybersecurity/openais-internal-ai-details-stolen-2023-breach-nyt-reports-2024-07-05/

--------------------------------------------------

Requirement: Establish a robust insider threat detection program
OpenAI has taken several steps since the beginning of 2023 to establish a robust insider threat detection program:

1. **Hiring an Insider Risk Investigator**: OpenAI is currently seeking a "seasoned Insider Risk Investigator" to help identify and protect against internal security threats.
2. **Cybersecurity and Insider Threat Safeguards**: OpenAI, along with other leading tech firms, has committed to investing in cybersecurity and insider threat safeguards to prevent unreleased model weights from falling into the wrong hands.
3. **Bug Bounty Program**: OpenAI has an ongoing bug bounty program that rewards security researchers and white hat hackers for reporting weaknesses, which helps in identifying potential insider threats.

### Links Used:
- https://openai.com/global-affairs/our-approach-to-frontier-risk/
- https://www.ccn.com/news/technology/openai-hiring-insider-risk-investigator-collaborating-white-house/

--------------------------------------------------

Requirement: Limiting access to model weights to those whose job function requires it
OpenAI has not explicitly mentioned limiting access to model weights to those whose job function requires it in the provided sources since the beginning of 2023. However, there are discussions and concerns about the governance and safety of AI models, which might indirectly relate to such practices.

- **Concerns about AI Governance**: The Center for AI and Digital Policy (CAIDP) has urged the Federal Trade Commission to investigate OpenAI and establish necessary safeguards for its models, including potential measures to control access to sensitive AI technologies.
- **Safety and Alignment**: OpenAI has been focusing on safety and alignment issues, including the development of early warning systems for LLM-aided biological threat creation and practices for governing agentic AI systems.

For more specific information on access control measures, further detailed updates from OpenAI or regulatory actions would be necessary. The provided sources do not explicitly mention any specific policies or actions taken by OpenAI to limit access to model weights based on job function since the beginning of 2023.

--------------------------------------------------

Requirement: Establish bounties, contests, or prizes
OpenAI has established a Bug Bounty Program since the beginning of 2023 to recognize and reward security researchers who contribute to keeping their technology secure. This program includes incentives and rewards for identifying vulnerabilities, with cash prizes ranging from $200 to $20,000 based on the severity and impact of the reported issues.

Additionally, OpenAI has introduced the ChatGPT Feedback Contest, which offers $500 in OpenAI API credits for submitting feedback on problematic outputs through ChatGPT.

### Links:
- [OpenAI's Bug Bounty Program](https://openai.com/index/bug-bounty-program/)
- [ChatGPT Feedback Contest: Official Rules](https://cdn.openai.com/chatgpt/chatgpt-feedback-contest.pdf)

--------------------------------------------------

Requirement: Include AI systems in their existing bug bounty programs
OpenAI has included AI systems in their existing bug bounty programs since the beginning of 2023. Here are the relevant links:

- **OpenAI Bug Bounty Program**: OpenAI has partnered with Bugcrowd to manage the submission and reward process, inviting security researchers to report vulnerabilities and bugs in their systems. The program offers cash rewards ranging from $200 to $20,000 based on the severity and impact of the reported issues.
- **Coordinated Vulnerability Disclosure Policy**: OpenAI encourages responsible vulnerability research and disclosure, emphasizing the importance of security in their mission. They provide detailed guidelines for participation in the Bug Bounty Program.
- **AI Safety Bounties**: While not exclusively focused on bug bounty programs, OpenAI's efforts in AI safety include initiatives like the ChatGPT Feedback Contest, which aimed to gather feedback on problematic outputs from ChatGPT. This contest was part of their broader efforts to enhance AI safety.

--------------------------------------------------

Requirement: Robust provenance or watermarking for audio
OpenAI has incorporated audio watermarking into its Voice Engine, a custom voice model, which is currently in a limited research preview. Additionally, they have developed a text watermarking method and are researching metadata as a text provenance method.

### Links:
- [OpenAI's content provenance work](https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online/)
- [OpenAI's commitment to robust mechanisms for audio or visual content](https://openai.com/index/moving-ai-governance-forward/)

--------------------------------------------------

Requirement: Robust provenance or watermarking for visual content
OpenAI has taken several steps since the beginning of 2023 to implement robust provenance and watermarking for visual content:

1. **Released a Provenance Classifier**: OpenAI announced the release of a provenance classifier in October 2023, which is over 99% accurate at identifying images generated by DALLÂ·E 3 and maintains over 95% accuracy even after common modifications like cropping, resizing, or JPEG compression.

2. **Developed Text Watermarking**: OpenAI has researched text watermarking methods, which have been highly accurate but are less robust against globalized tampering. They are also exploring metadata as a text provenance method, which is cryptographically signed and has no false positives.

3. **Implemented Tamper-Resistant Watermarking**: OpenAI is introducing new tools to help researchers study content authenticity and is implementing tamper-resistant watermarking for digital content like audio.

4. **Participated in Content Provenance Work**: OpenAI is part of the Coalition for Content Provenance and Authenticity Steering Committee and is actively researching and developing new provenance methods to enhance the integrity of digital content.

### Links Used:
- https://partnershiponai.org/openai-framework-case-study/
- https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online/
- https://innovating.news/article/watermarks-are-just-one-of-many-tools-needed-for-effective-use-of-ai-in-news/

--------------------------------------------------

Requirement: Develop tools or APIs to determine if a particular piece of content was created within their tools
OpenAI has made several advancements since the beginning of 2023 that fit under developing tools or APIs to determine if a particular piece of content was created within their tools. Here are the key points:

1. **Enhanced Multimodal Capabilities**:
   - OpenAI has expanded its multimodal capabilities, integrating vision and text-to-speech functionalities. This includes the ability for GPT-4 Turbo to process images and the integration of DALLÂ·E 3 for image creation.

2. **New Assistants API**:
   - OpenAI has introduced the Assistants API, which empowers developers to build applications that can perform complex tasks and call on various models and tools. This API is a step towards creating more intuitive, agent-like applications that can maintain state and execute functions such as code execution and knowledge retrieval.

3. **GPT Store**:
   - The upcoming GPT store will allow users to sell tools they create, incentivizing the development of useful AI-powered applications. This suggests a focus on creating and identifying AI-generated content.

4. **API Documentation and Examples**:
   - OpenAI has provided detailed API documentation and examples, including curl commands for authentication and API requests, which can help developers integrate their tools effectively.

These advancements indicate a significant focus on developing and utilizing AI tools to create and identify AI-generated content.

--------------------------------------------------

Requirement: Work with industry peers and standards-setting bodies as appropriate towards developing a technical framework to help users distinguish audio or visual content generated by users from audio or visual content generated by AI
OpenAI has made several commitments and partnerships since the beginning of 2023 that align with developing a technical framework to distinguish AI-generated content. Here are the key points:

1. **Content Integration and AI Features Co-Development**: OpenAI has partnered with the Financial Times to integrate FT content into ChatGPT, enhancing response accuracy and credibility. This collaboration aims to create new AI tools for digital news consumption and provide high-quality, well-researched information.

2. **Data Integrity and Transparency**: OpenAI's Supplier Code of Conduct emphasizes maintaining transparent and accurate business records, including data integrity and confidentiality. This ensures that AI-generated content can be distinguished from human-generated content through robust provenance and watermarking.

3. **AI Governance and Safety**: OpenAI has made voluntary commitments to reinforce AI safety, security, and trustworthiness. This includes developing mechanisms to enable users to understand if audio or visual content is AI-generated, such as robust provenance, watermarking, or both.

### Links Used:
- [OpenAI and Financial Times Partnership](https://foundationinc.co/lab/openai-partnerships-list/)
- [OpenAI Supplier Code of Conduct](https://openai.com/policies/supplier-code/)
- [OpenAI Governance Commitments](https://openai.com/index/moving-ai-governance-forward/)

--------------------------------------------------

Requirement: Report capabilities
OpenAI has taken several steps since the beginning of 2023 to report capabilities and ensure safety:

1. **Published System Cards**:
   - OpenAI published system cards for new AI systems, including the GPT-4 System Card and DALL-E 2 System Card, which inform readers about key factors impacting the systemâs behavior.
   - A System Card was also published prior to releasing DALL-E 3 in ChatGPT, detailing its capabilities and limitations.

2. **Bug Bounty Program**:
   - OpenAI announced a bug bounty program to recognize and reward individuals who report security vulnerabilities in their systems. The program offers rewards ranging from $200 to $20,000 for low-severity and exceptional discoveries, respectively.

3. **Red Teaming Efforts**:
   - OpenAI conducted extensive red teaming with over 100 external red teamers, representing 29 different countries and speaking 45 different languages. This effort tested the model's capabilities and safety mitigations across multiple phases.

4. **Safety Commitments and Frameworks**:
   - OpenAI joined other leading AI labs in making voluntary commitments to safely develop and deploy frontier AI models. This includes a pledge to publish safety frameworks like the Preparedness Framework, which they developed and adopted last year.

5. **GPT-4o System Card**:
   - The GPT-4o System Card provides a detailed look at its capabilities, limitations, and safety evaluations across multiple categories. It includes information on the model's pre-training data, external red teaming efforts, and ongoing safety measurements.

### Links Used:
- https://openai.com/global-affairs/our-approach-to-frontier-risk/
- https://openai.com/index/gpt-4o-system-card/
- https://openai.com/index/openai-safety-update/

--------------------------------------------------

Requirement: Report limitations
OpenAI has reported several limitations since the beginning of 2023, including:

1. **Knowledge Cutoff**: The knowledge cutoff for some models is still in September 2021, which is not handy for many cases.
2. **Model Performance**: Some users have reported a decrease in model performance, with the model appearing "lazier" and preferring to provide overviews rather than full code as requested.
3. **Compute Limitations**: OpenAI has compute limitations, which has led to the use of ablated models that cannot reason or provide significant improvements.

### Links Used:
- https://community.openai.com/t/anger-and-discouragement-towards-openai/577210
- https://community.openai.com/t/now-thats-behind-us-can-openai-stop-competing-with-its-developers/519985
- https://community.openai.com/t/when-will-openai-finally-bring-the-knowledge-to-2023/369918

--------------------------------------------------

Requirement: Report domains of appropriate and inappropriate use
OpenAI has made several updates and announcements since the beginning of 2023 that might fit under "Report domains of appropriate and inappropriate use." Here are the key points:

1. **Knowledge Cutoff Update**:
   - The knowledge cutoff for GPT-4 has been updated to January 2022, while GPT-3.5-T still has a cutoff of September 2021.

2. **Product Releases and Improvements**:
   - OpenAI has introduced new tools and improvements, such as GPT-4o and more tools for free users, as well as enhancements to the fine-tuning API and custom models program.

3. **Classifier for Human-AI Text**:
   - OpenAI launched a new classifier to distinguish between text written by humans and text written by AI, which is publicly available for feedback.

4. **Partnerships and Collaborations**:
   - OpenAI has partnered with companies like Microsoft and BuzzFeed to advance AI technology and its applications.

5. **Safety and Security Initiatives**:
   - The OpenAI Board formed a Safety and Security Committee to address concerns related to AI safety and alignment.

### Links Used:
- https://community.openai.com/t/new-knowledge-cutoff-date/382962
- https://timelines.issarice.com/wiki/Timeline_of_OpenAI
- https://openai.com/news?author=tom-brown

--------------------------------------------------

Requirement: Publish transparency reports
OpenAI has not explicitly published a transparency report since the beginning of 2023. However, they have shared their safety practices and risk mitigation measures in their safety update, which includes empirical model red-teaming and testing before release, security and access control measures, and partnering with governments to inform AI safety policies.

For transparency reporting specifically, OpenAI's practices do not align with the detailed transparency reports recommended for generative AI providers, which include explaining how harmful content is defined, reporting its frequency, and describing mitigations implemented. 

### Links Used:
- https://openai.com/index/openai-safety-update/
- https://knightcolumbia.org/blog/generative-ai-companies-must-publish-transparency-reports

--------------------------------------------------

Requirement: Report safety evaluations
OpenAI has taken several steps since the beginning of 2023 to report safety evaluations:

1. **Partnership with Thorn's Safer**: OpenAI partnered with Thorn's Safer to detect, review, and report Child Sexual Abuse Material to the National Center for Missing and Exploited Children. This partnership was announced in May 2023, but the exact date is not specified in the provided link.

2. **Voluntary Commitments for AI Safety**: OpenAI joined other leading AI labs in making voluntary commitments to promote safety, security, and trust in AI. These commitments included a range of risk areas, including frontier risks, and were announced on July 21, 2023.

3. **Red Teaming and Safety Evaluations**: OpenAI conducted extensive red-teaming for its models, including GPT-4, to evaluate safety. This process involved external red teamers testing the models for various frontier risks such as aiding the development of nuclear, radiological, biological, and chemical weapons (CBRN), increasing cyber risk, risks stemming from tool use, and self-replication capabilities.

4. **System Cards for Model Safety**: OpenAI has been publishing system cards that detail the capabilities and risks of its models. For example, the system card for GPT-4o includes the results of external red teaming, frontier risk evaluations according to the Preparedness Framework, and an overview of risk mitigations built into the systems.

5. **Safety Frameworks and Transparency**: OpenAI has been sharing its safety frameworks and practices, including the Preparedness Framework, which outlines the safety measures integrated into the development process. The company also emphasizes transparency by publishing system cards and sharing safety work related to its models.

### Links Used:
- https://openai.com/index/openai-safety-update/
- https://openai.com/index/gpt-4o-system-card/
- https://openai.com/global-affairs/our-approach-to-frontier-risk/

--------------------------------------------------

Requirement: Report on societal risks
OpenAI has taken several steps since the beginning of 2023 to address societal risks associated with AI:

1. **Partnership with Thorn's Safer**: OpenAI partnered with Thorn's Safer to detect, review, and report Child Sexual Abuse Material to the National Center for Missing and Exploited Children if users attempt to upload it to their image tools.

2. **Election Integrity Measures**: OpenAI collaborated with governments and stakeholders to prevent abuse, ensure transparency on AI-generated content, and improve access to accurate voting information. They introduced a tool for identifying images created by DALLÂ·E 3, joined the steering committee of the Content Authenticity Initiative (C2PA), and incorporated C2PA metadata in DALLÂ·E 3 to help people understand the source of media they find online. ChatGPT now directs users to official voting information sources in the U.S. and Europe.

3. **Investment in Impact Assessment and Policy Analysis**: OpenAI has been influential in research, industry norms, and policy, including early work on measuring CBRN risks associated with AI systems and estimating the extent to which different occupations and industries might be impacted by language models. They also published pioneering work on how society can best manage associated risks, such as assessing the implications of language models for influence operations.

4. **Frontier AI Safety Commitments**: OpenAI joined other leading AI labs in making voluntary commitments to promote safety, security, and trust in AI. These commitments include pre-deployment safety evaluations and red-teaming, as well as investing in responsible practices through the rollout of voice and image analysis capabilities in ChatGPT.

5. **Addressing Regulatory Concerns**: OpenAI faced regulatory scrutiny, with the Center for AI and Digital Policy (CAIDP) urging the Federal Trade Commission (FTC) to investigate OpenAI and establish necessary safeguards for ChatGPT. The FTC has opened an investigation into OpenAI's practices, highlighting concerns over bias, deception, privacy, and public safety.

### Links Used:
- https://openai.com/index/openai-safety-update/
- https://www.caidp.org/cases/openai/
- https://openai.com/global-affairs/our-approach-to-frontier-risk/

--------------------------------------------------

Requirement: Report on adversarial testing used to determine appropriateness of deployment
OpenAI has taken several steps since the beginning of 2023 to ensure the appropriateness of deployment through adversarial testing:

1. **Public Release of DALL-E 3**: OpenAI conducted pre-deployment safety evaluations and red-teaming for DALL-E 3, a new frontier model, as part of their voluntary commitments to promote safety and security in AI.

2. **GPT-4 System Card**: OpenAI built internal quantitative evaluations and adversarial testing for GPT-4, including assessments for categories against their safety mitigations. They also continued to learn from deployment and update their models to make them safer and more aligned with human preferences.

3. **Red Teaming Network**: OpenAI established the Red Teaming Network, a community of experts that helps inform their risk assessment and mitigation efforts. This network includes diverse domains and perspectives, ensuring comprehensive evaluation of AI systems.

4. **GPT-4o System Card**: For GPT-4o, OpenAI assessed and mitigated potential risks through a combination of methods spanning pre-training, post-training, product development, and policy. They aligned the model to human preferences, red-teamed the models, and added product-level mitigations like monitoring and enforcement.

5. **Adversarial Attacks on LLMs**: OpenAI is aware of the threat model for adversarial attacks on large language models (LLMs), including white-box and black-box attacks. They recognize the need for high-quality classifiers to judge the success of these attacks, which often demand human review.

### Links Used:
- https://openai.com/global-affairs/our-approach-to-frontier-risk/
- https://cdn.openai.com/papers/gpt-4-system-card.pdf
- https://openai.com/index/red-teaming-network/
- https://openai.com/index/gpt-4o-system-card/
- https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/

--------------------------------------------------

Requirement: Empower trust and safety teams
OpenAI has taken several steps since the beginning of 2023 to empower trust and safety teams:

1. **Formation of the Safety and Security Committee (SSC)**: OpenAI formed the SSC in May 2024, which is responsible for making recommendations on critical safety and security decisions for all OpenAI projects. The committee includes directors Bret Taylor, Adam D'Angelo, Nicole Seligman, and Sam Altman (CEO), along with technical and policy experts like Aleksander Madry, Lilian Weng, John Schulman, Matt Knight, and Jakub Pachocki.

2. **Enhanced Safety Measures**: OpenAI has elevated the SSC to an independent Board oversight body, giving it broader powers and independence. The committee will have the authority to delay model releases if safety concerns are not adequately addressed.

3. **Increased Transparency**: OpenAI has committed to more comprehensive explanations of its safety work. This includes the release of detailed safety evaluations, external red teaming exercises, and rigorous Preparedness Framework evaluations, as seen in the 43-page System Card for the o1 model family.

4. **Collaborations with External Organizations**: OpenAI is developing new partnerships with third-party safety organizations and non-governmental labs for independent model assessments. The company has also struck agreements with the U.S. and U.K. AI Safety Institutes to research emerging AI safety risks and establish standards for trustworthy AI.

5. **Red Teaming and Model Evaluations**: OpenAI conducts extensive red-teaming exercises to evaluate new models for safety risks, including CBRN risks, cyber risks, tool use risks, and self-replication capabilities. This process involves testing models with external red-teamers to identify potential vulnerabilities.

6. **Investment in Alignment and Preparedness**: OpenAI has created two new teams: Superalignment and Preparedness. The Superalignment team aims to solve the problem of aligning superintelligence within four years, while the Preparedness team focuses on identifying, tracking, and preparing for frontier risks associated with advanced AI models.

These efforts demonstrate OpenAI's commitment to enhancing trust and safety in AI development and deployment.

--------------------------------------------------

Requirement: Advance AI safety research
OpenAI has taken several steps to advance AI safety research since the beginning of 2023:

1. **Voluntary Commitments**: OpenAI joined other leading AI labs in making voluntary commitments to promote safety, security, and trust in AI on July 21, 2023.
2. **Preparedness Framework**: OpenAI developed and adopted a Preparedness Framework to evaluate model safety before release, ensuring that models do not cross a "Medium" risk threshold until sufficient safety interventions are implemented.
3. **Public Release of Safety Frameworks**: OpenAI published its safety frameworks, including the Preparedness Framework, and pledged to share information about risk mitigation measures.
4. **Enhanced Transparency**: OpenAI unveiled a novel method to enhance the transparency of its systems by initiating a dialogue between two AI models, making the dominant model's reasoning more understandable to humans.
5. **Collaboration with AI Safety Institutes**: OpenAI agreed to provide early access to its next major generative AI model for safety testing to the U.S. AI Safety Institute and the U.K.âs AI safety body.
6. **Investment in Safety Research**: OpenAI dedicated 20% of its compute to safety research and voided non-disparagement clauses to encourage whistleblowing, addressing criticisms about prioritizing flashy advancements over safety.

### Links Used:
- https://openai.com/index/openai-safety-update/
- https://www.wired.com/story/openai-safety-transparency-research/
- https://techcrunch.com/2024/07/31/openai-pledges-to-give-u-s-ai-safety-institute-early-access-to-its-next-model/
- https://openai.com/global-affairs/our-approach-to-frontier-risk/

--------------------------------------------------

Requirement: Advance privacy
OpenAI has taken several steps to advance privacy since the beginning of 2023:

1. **Encryption and Access Controls**: OpenAI encrypts all data at rest (AES-256) and in transit (TLS 1.2+), and uses strict access controls to limit who can access data.
2. **Data Ownership and Control**: OpenAI emphasizes that users own and control their data, with no training on business data by default. Users also control how long their data is retained and decide who has access.
3. **Security Audits**: OpenAI has been audited for SOC 2 compliance for ChatGPT Enterprise and API, ensuring comprehensive compliance with security standards.
4. **Consumer Privacy Management**: Users can manage their data by controlling whether they contribute to future model improvements in their settings. Temporary chats are not used to train models, and customer data is not used to train models by default.
5. **Privacy Policy Updates**: OpenAI has updated its privacy policy to include clearer information about data processing, retention, and user rights, such as the right to know, request deletion, correct, and be free from certain practices.

These measures highlight OpenAI's commitment to advancing privacy and security in its services. 

### Links Used:
- https://openai.com/enterprise-privacy/
- https://openai.com/consumer-privacy/
- https://openai.com/policies/jun-2023-privacy-policy/

--------------------------------------------------

Requirement: Protect children
OpenAI has not explicitly stated any specific actions taken since the beginning of 2023 that directly fit under "Protect children." However, the company has emphasized its commitment to safety and alignment in AI development, which includes measures to mitigate potential risks, including those related to children's safety. Here are the relevant links:

- **Safety & Alignment**: OpenAI has been working on various initiatives to ensure the safe development of AI, including the development of new AI models and the exploration of how to better use human feedback to train AI systems. This includes discussions on the alignment problem and the potential risks of unaligned AGI.
- **Partnerships and Integrations**: OpenAI has announced partnerships with companies like Apple to integrate ChatGPT into various experiences, which could potentially include safety features for children.
- **Research and Development**: OpenAI has been advancing its AI models, such as GPT-4, which includes improvements in cost-efficient intelligence and the development of new video generation models. These advancements are part of the company's broader efforts in AI safety and alignment.

While these efforts are not specifically targeted at protecting children, they contribute to a broader context of ensuring AI safety and alignment, which indirectly supports child safety.

--------------------------------------------------

Requirement: support research and development of frontier AI systems that can help meet societyâs greatest challenges, such as climate change mitigation and adaptation, early cancer detection and prevention, and combating cyber threats
OpenAI has engaged in several initiatives since the beginning of 2023 that support research and development of frontier AI systems addressing societal challenges, including climate change mitigation and adaptation. Here are the key activities:

1. **Sustainable Development and AI**:
   - OpenAI has discussed sustainable cloud infrastructure with Azure and explored sustainable technologies for lowering the carbon footprint of businesses. This includes using Azure infrastructure, which has been carbon neutral since 2012, and addressing Scope 1, 2, and 3 emissions in the value chain.

2. **OpenAI X Climate Change Hackathon**:
   - Although this event occurred in 2022, it highlights OpenAI's commitment to addressing climate change. The hackathon aimed to explore how current AI models could accelerate solutions to climate change by partnering with innovative non-profit organizations and startups.

3. **Accelerating ChatGPT 5 Launch and AGI by 2029**:
   - OpenAI has proposed accelerating the development of ChatGPT 5 and targeting AGI by 2029 to address global challenges like healthcare, climate change, and economic inequality. This initiative aims to improve living conditions worldwide through AI-driven solutions.

4. **OpenAI GPT-4: Fighting Climate Change**:
   - OpenAI's GPT-4 has been highlighted for its potential in fighting climate change by providing insights and solutions to environmental issues. The advanced language model can analyze data and generate human-like responses, making it a valuable tool for environmental organizations.

### Links Used:
- https://community.openai.com/t/sustainable-development-and-ai/377448
- https://community.openai.com/t/title-accelerating-chatgpt-5-launch-and-agi-by-2029-for-global-impact/928510
- https://nandbox.com/openai-gpt-4-fighting-climate-change-with-the-unique-ai-model/

--------------------------------------------------

Requirement: support initiatives that foster the education and training of students and workers to prosper from the benefits of AI
Since the beginning of 2023, OpenAI has taken several initiatives to support education and training in AI:

1. **Released a Teaching Guide for Educators**: OpenAI has released a guide to help educators incorporate ChatGPT into their instruction, providing examples and answers to frequently-asked questions.

2. **Data Partnerships**: OpenAI has introduced Data Partnerships, where they collaborate with organizations to produce public and private datasets for training AI models. This initiative aims to make AI models more useful by including content from various domains.

3. **Enhancements to Fine-Tuning API and Custom Models Program**: OpenAI has improved its fine-tuning API and expanded its custom models program, allowing for more tailored AI solutions.

These initiatives demonstrate OpenAI's commitment to fostering education and training in AI, ensuring that both educators and students can benefit from the advancements in AI technology.

### Links Used:
- [OpenAI Data Partnerships](https://openai.com/index/data-partnerships/)
- [OpenAI Releases ChatGPT Teaching Guide](https://www.eschoolnews.com/featured/2023/10/19/teaching-with-chatgpt-guide/)
- [OpenAI Enhancements to Fine-Tuning API and Custom Models Program](https://openai.com/)

--------------------------------------------------

Requirement: support initiatives that help citizens understand the nature, capabilities, limitations, and impact of the technology
OpenAI has taken several initiatives since the beginning of 2023 to support understanding of AI technology:

1. **OpenAI Safety Update**: OpenAI has been actively sharing its safety practices and engaging in discussions about AI safety, including at the AI Seoul Summit. They emphasize a balanced, scientific approach to integrating safety measures into the development process from the outset.

2. **Frontier Risk Mitigation**: OpenAI has made voluntary commitments to promote safety, security, and trust in AI. This includes developing a Preparedness Framework and publishing system cards for new models like DALL-E 3, which provide key information about the system's behavior.

3. **Bug Bounty Program**: OpenAI has launched a bug bounty program to recognize and reward individuals who report security vulnerabilities in their systems. This program is available through Bugcrowd and offers rewards ranging from $200 to $20,000.

4. **Partnership with Moderna**: OpenAI has partnered with Moderna to deploy ChatGPT Enterprise, which is accelerating the development of life-saving treatments. This partnership aims to empower every function within Moderna with AI, creating novel use cases and accelerating the impact of every team.

These initiatives aim to enhance transparency and safety in AI development and deployment, helping citizens understand the nature, capabilities, limitations, and impact of the technology.

### Links Used:
- https://openai.com/global-affairs/our-approach-to-frontier-risk/
- https://openai.com/index/openai-safety-update/
- https://openai.com/es-419/index/moderna/

--------------------------------------------------

