Internal red-teaming
External red-teaming
Red teaming related to each of the risk areas
Info sharing with companies
Info sharing with government
Establish or join a forum or mechanism 
Sharing information in the specific risk areas
Invest in cybersecurity / store and working with the weights in an appropriately secure environment
Establish a robust insider threat detection program
Limiting access to model weights to those whose job function requires it
Establish bounties, contests, or prizes
Include AI systems in their existing bug bounty programs
Robust provenance or watermarking for audio
Robust provenance or watermarking for visual content
Develop tools or APIs to determine if a particular piece of content was created within their tools
Work with industry peers and standards-setting bodies as appropriate towards developing a technical framework to help users distinguish audio or visual content generated by users from audio or visual content generated by AI
Report capabilities
Report limitations
Report domains of appropriate and inappropriate use
Publish transparency reports
Report safety evaluations
Report on societal risks
Report on adversarial testing used to determine appropriateness of deployment
Empower trust and safety teams
Advance AI safety research
Advance privacy
Protect children
support research and development of frontier AI systems that can help meet societyâ€™s greatest challenges, such as climate change mitigation and adaptation, early cancer detection and prevention, and combating cyber threats
support initiatives that foster the education and training of students and workers to prosper from the benefits of AI
support initiatives that help citizens understand the nature, capabilities, limitations, and impact of the technology